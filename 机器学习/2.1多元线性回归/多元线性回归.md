# 多元线性回归



1. ***基本概念：***

    所谓多元线性回归其实就是自变量的个数变多了，之前的简单线性回归方程可以表示为：![y=b_0 +bx](https://math.jianshu.com/math?formula=y%3Db_0%20%2Bbx),那么在多元中则是![y=b_0+b_1x_1+b_2x_2+...+b_nx_n](https://math.jianshu.com/math?formula=y%3Db_0%2Bb_1x_1%2Bb_2x_2%2B...%2Bb_nx_n)。

2. ***线性回归的几个前置条件：***

    在正式使用多元线性回归之前，我们先谈谈关于线性回归的几个前置条件，首先，在线性回归中有几个重要的假设如下所示：

    1. Linearity 线性 （数据呈线性关系）
    2. Homoscedasticity 同方差性（数据要有相同的方差）
    3. Multivariate normality 多元正态分布 （数据要呈现多元正态分布）
    4. Independence of errors 误差独立 （各个维度上的误差相互独立）
    5. Lack of multicollinearity 无多重共线性 （没有一个自变量和另外的自变量存在线性关系）

3. ***如何构建一个多元线性回归模型：***

    在实际应用中，往往会遇到对于一个因变量y，有很多的自变量x1，x2等等，但这些自变量不是所有的都是对这个y的预测很有帮助因素，我们需要从其中剔除掉无用的元素来得到最合适的模型，那么此时就有一个问题，如何来选择这些自变量呢？这里有五种方法来建立模型：

    1. All-in 

    2. Backward Elimination 反向淘汰

    3. Forward Selection 顺向选择

    4. Bidirectional Elimination 双向淘汰

    5. Score Comparison 信息量比较

4. ***数据处理：***

    ```python
    # 导入数据
    dataset = pd.read_csv('50_Startups.csv')
    x = dataset.iloc[:, :-1].values
    y = dataset.iloc[:, 4].values
    
    label_encoder = LabelEncoder()
    x[:, 3] = label_encoder.fit_transform(x[:, 3])
    ct = ColumnTransformer([("Country", OneHotEncoder(), [3])], remainder='passthrough')
    x = ct.fit_transform(x)
    # 虚拟变量陷阱
    x = x[:, 1:]
    
    # 划分数据
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
    return x_train, x_test, y_train, y_test
    ```

5. ***建立模型：***

    ```python
    x_train, x_test, y_train, y_test = self.data_preprocessing_template()
    from sklearn.linear_model import LinearRegression
    # 定义回归器
    regressor = LinearRegression()
    # 拟合回归器
    regressor.fit(x_train, y_train)
    # 用回归器预测 x_test
    y_pred = regressor.predict(x_test)
    # 反向淘汰自变量
    import statsmodels.api as sm
    x_train = np.append(arr=np.ones((40, 1)), values=x_train, axis=1)
    # 最佳自变量选择
    x_opt = x_train[:, [0, 1, 2, 3, 4, 5]]
    x_opt = x_opt.astype(np.float64)
    regressor_ols = sm.OLS(endog=y_train, exog=x_opt).fit()
    print(regressor_ols.summary())
    # 根据P值 大于0.1 去除相应自变量 1，2，4
    x_opt = x_train[:, [0, 3, 5]]
    x_opt = x_opt.astype(np.float64)
    regressor_ols = sm.OLS(endog=y_train, exog=x_opt).fit()
    print(regressor_ols.summary())
    ```

6. 输出结果：

    ![1](.\images\1.jpg)

    根据这个结果可以得到的结论就是一家公司的收益主要跟公司的研发投入有关。当然其实其中也有其他的自变量对应了很低的P值，如果自己一步步运行这段代码会发现行政的投入对应的P值也只有0.07不算很高，如何更好的判断线性回归模型的优劣后面还会有其他的方法来判断。
    

